# Code Refactoring Assessment

This repository is made as a qualitative assessment of human refactoring skills for the purpose of comparison with those of AI language models.

# Rules

1. The given function in each excercise must be refactored to fix any problems it might have. (Readability, vulnerability, exception, data validation, etc.)

2. Considering the goal of this excercise is to see the differences between the human approach to refactoring existing code snippets as opposed to the AI approach, the takers of this assessment are not to use LLMs to help themselves. However, they are allowed to use internet (i.e. Stack Overflow, official documentation, internet tutorials...)

3. Data sent to the function as a parameter is not validated. However, the expected format of the parameters will be provided.

4. The refactored functions are not supposed to throw any exceptions while keeping the original functionality.

5. All edge cases must be covered.

6. The takers of this assessment should time themselves for each excercise and write down the exact time needed to solve each one.

7. No external libraries can be imported or used

8. All of the .py file can be submitted for review by email to niko.rimac@fer.hr
